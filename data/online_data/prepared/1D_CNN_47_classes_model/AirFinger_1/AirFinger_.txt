Features: x & y normalized with maxabscaler sample by sample and resampled to 60 points
train: 80%	test: 20%
epochs = 50
batch_size = 16
learning_rate = 0.01
acc = 0.7480748074807481

train_acc=	0.5350855745721271	 test_acc=	0.44884488448844884
train_acc=	0.563080684596577	 test_acc=	0.45654565456545654
train_acc=	0.5693154034229829	 test_acc=	0.46534653465346537
train_acc=	0.5786063569682152	 test_acc=	0.4598459845984598
train_acc=	0.5788508557457213	 test_acc=	0.47414741474147415
train_acc=	0.5858190709046455	 test_acc=	0.45434543454345433
train_acc=	0.45041947462522347	 test_acc=	0.4273927392739274
train_acc=	0.6488791087883372	 test_acc=	0.6353135313531353
train_acc=	0.6765231742538853	 test_acc=	0.6435643564356436
train_acc=	0.7214963553844038	 test_acc=	0.6826182618261826
train_acc=	0.7748590290193921	 test_acc=	0.724972497249725
train_acc=	0.768945124467061	 test_acc=	0.7271727172717272
train_acc=	0.787787099436116	 test_acc=	0.7365236523652365
train_acc=	0.7875120341081007	 test_acc=	0.7563256325632564
train_acc=	0.7960390592765781	 test_acc=	0.7596259625962596
train_acc=	0.8004401045248246	 test_acc=	0.7596259625962596
train_acc=	0.8070416723971944	 test_acc=	0.7552255225522553
train_acc=	0.8100673910053637	 test_acc=	0.7519251925192519
train_acc=	0.8020904964929171	 test_acc=	0.7513751375137514
train_acc=	0.808554531701279	 test_acc=	0.7662266226622663
train_acc=	0.8081419337092559	 test_acc=	0.7651265126512651
train_acc=	0.8117177829734562	 test_acc=	0.7491749174917491
train_acc=	0.8188694815018567	 test_acc=	0.7491749174917491
train_acc=	0.8115802503094485	 test_acc=	0.742024202420242
train_acc=	0.8128180442855178	 test_acc=	0.7475247524752475
train_acc=	0.8249209187181956	 test_acc=	0.7601760176017601
train_acc=	0.8232705267501031	 test_acc=	0.7552255225522553
train_acc=	0.8250584513822032	 test_acc=	0.7546754675467546
train_acc=	0.8216201347820107	 test_acc=	0.7629262926292629
train_acc=	0.8198322101499106	 test_acc=	0.7326732673267327
train_acc=	0.8367487278228579	 test_acc=	0.7277227722772277
train_acc=	0.8290468986384266	 test_acc=	0.7667766776677668
train_acc=	0.82107000412598	 test_acc=	0.7541254125412541
train_acc=	0.8250584513822032	 test_acc=	0.7447744774477447
train_acc=	0.8309723559345344	 test_acc=	0.7491749174917491
train_acc=	0.83303534589465	 test_acc=	0.7431243124312431
train_acc=	0.8331728785586576	 test_acc=	0.7403740374037404
train_acc=	0.8317975519185806	 test_acc=	0.7585258525852585
train_acc=	0.8360610645028195	 test_acc=	0.7491749174917491
train_acc=	0.8254710493742263	 test_acc=	0.740924092409241
train_acc=	0.8320726172465961	 test_acc=	0.7513751375137514
train_acc=	0.835785999174804	 test_acc=	0.7343234323432343
train_acc=	0.8272589740063265	 test_acc=	0.7722772277227723
train_acc=	0.8323476825746114	 test_acc=	0.757975797579758
train_acc=	0.8334479438866731	 test_acc=	0.7304730473047305
train_acc=	0.8359235318388117	 test_acc=	0.740924092409241
train_acc=	0.8366111951588502	 test_acc=	0.7387238723872387
train_acc=	0.8366111951588502	 test_acc=	0.7255225522552256
train_acc=	0.837161325814881	 test_acc=	0.742024202420242
train_acc=	0.8304222252785036	 test_acc=	0.7381738173817382
train_acc=	0.8401870444230505	 test_acc=	0.742024202420242
train_acc=	0.8388117177829735	 test_acc=	0.7475247524752475
train_acc=	0.8383991197909504	 test_acc=	0.742024202420242
train_acc=	0.8414248383991197	 test_acc=	0.7403740374037404
train_acc=	0.8337230092146884	 test_acc=	0.7370737073707371
train_acc=	0.8331728785586576	 test_acc=	0.7480748074807481
